---
title: "RRBoost: a robust boosting algorithm for regression problems"
author: "Xiaomeng Ju and Matias Salibian Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---

This repository contains `R` code implementing the
robust boosting algorithm [paper URL](https://github.com). 
This method provides an outlier-robust 
fit for non-parametric regression models. 

The package can be installed as follows:
```{r initial, eval=FALSE, message=FALSE, warning=FALSE}
devtools::install_github("xmengju/RRBoost")
```

We will illustrate the use of the package with the `airfoil` 
dataset. It consists of `n = 1503` observations with `p = 5`
explanatory variables, and the goal is to predict the
response variable `y`. We load the data and look at the 
first few observations:
```{r print}
library(RRBoost)
data(airfoil)
head(airfoil)
```
In order to train our predictor, we split the data set into
a `training` set (with 60% of the available data), a
`validation` set and a `test` set (both with 20%
of the data). We first randomly select the observations
for each of these three sets:
```{r sampleidx}
n <- nrow(airfoil) 
n0 <- floor( 0.2 * n ) 
set.seed(123)
idx_test <- sample(n, n0)
idx_train <- sample((1:n)[-idx_test], floor( 0.6 * n ) )
idx_val <- (1:n)[ -c(idx_test, idx_train) ] 
```
<!-- To illustrate the robustness of the `RRBoost` fit, we  -->
<!-- add 20% of asymmetric outliers to the `training` and  -->
<!-- `validation`. We randomly select 20% of the observations -->
<!-- among these two sets combined, and perturb the response -->
<!-- variable by adding a `N( -20, 0.1^2)` random variable.  -->
<!-- ```{r addoutlier} -->
<!-- aircont <- airfoil -->
<!-- # indices of observations that may be contaminated -->
<!-- tmp <- c(idx_train, idx_val)  -->
<!-- # randomly sample 20% of them -->
<!-- nnc <- round( 0.2 * length(tmp) ) -->
<!-- outliers <- sample(tmp, nnc) -->
<!-- # shift the response variable with a N(-20, 0.1^2) r.v. -->
<!-- # aircont$y[outliers] <- aircont$y[outliers] + rnorm(nnc, -20, 0.1) -->
<!-- ``` -->
We now create the 
matrices of explanatory variables (`x`) and vectors of responses
(`y`) corresponding to this partition. 
<!-- Note that `ytrain` and `yval` may contain outliers. -->
```{r createsets}
xx <- airfoil[, -6]
yy <- airfoil$y
xtrain <- xx[ idx_train, ]
ytrain <- yy[ idx_train ]
xval <- xx[ idx_val, ]
yval <- yy[ idx_val ]
xtest <- xx[ idx_test, ]
ytest <- yy[ idx_test ]
```
The `Boost` function implements the `RRBoost` estimator, as 
well as the following previously proposed boosting algorithms:
`L2Boost`, `LADBoost`, `MBoost`, and `Robloss`.  

There are two choices for the initial fit used in the `RRBoost` 
algorithm: `median` (corresponds to a constant initial fit equal
to the median of the response variable in the training set), and
`LADTree` (the initial fit is an L1 Tree as proposed in ). 
To construct the L1 initial fit the user can select the
desired maximum depth and minimum leaf size, or use 
the function `Boost.validation` to 
select these parameters based  the performance on the 
validation set.  

We now train the `RRBoost` predictor using the following three
initial fits: `median`, an `LADTree` with parameters chosen 
a priori, and an `LADTree` selected using the validation set.
The performance of the resulting three predictors on the test
set will be measured using the root mean squared (prediction) 
error (set with the argument `error = "rmse"`). Other possible
options are: average absolute deviation (`aad`),
trimmed root mean squared (prediction) error (`trmse`) and 
a robust root mean squared (prediction) error (`rrmse`).
See the help page. 

The depth of the base learners in the boosting algorithm 
is set with the argument `max_depth` (below we use decision stumps: 
`max_depth = 1`). The argument `niter` specifies the number of
iterations (epochs) to be used (we set `niter = 1000`). 
Predictions (on the supplied test set) and variable importance
scores can be computed by setting `make_prediction =  TRUE` 
and `cal_imp = TRUE` in 
the argument `control`, as we do below. 

For the fit computed with an `LADTree` chosen a priori, 
we set its maximum depth to `max_depth_init = 2` and 
the minimum number of observations 
per node as `min_leaf_size_init = 20`. 

Finally, we also use as initial fit the best `LADTree`
among those constructed with all the possible combinations of
`max_depth_init_set` between 1 and 5 
(`max_depth_init_set = 1:5`)
and `min_leaf_size_init_set` equal to 10, 20 or 30 
(`min_leaf_size_init_set = c(10,20,30)`). 

```{r median, results="hide", cache=TRUE, message=FALSE}
model_RRBoost_median = Boost(x_train = xtrain, y_train = ytrain, 
                             x_val = xval, y_val = yval, 
                             x_test = xtest, y_test = ytest, 
                             type = "RRBoost", error = "rmse", 
                             y_init = "median", max_depth = 1, 
                             niter = 1000, 
                             control = Boost.control(make_prediction =  TRUE,
                                                     cal_imp = TRUE))

model_RRBoost_LADTree = Boost(x_train = xtrain, y_train = ytrain, 
                              x_val = xval, y_val = yval, 
                              x_test = xtest, y_test = ytest, 
                              type = "RRBoost", error = "rmse", 
                              y_init = "LADTree", max_depth = 1, 
                              niter = 1000, 
                              control = Boost.control(max_depth_init = 2,
                                                      min_leaf_size_init = 20,
                                                      make_prediction =  TRUE,
                                                      cal_imp = TRUE))

model_RRBoost_cv_LADTree = Boost.validation(x_train = xtrain, 
                                            y_train = ytrain, 
                                            x_val = xval, y_val = yval, 
                                            x_test = xtest, 
                                            y_test = ytest, 
                                            type = "RRBoost", 
                                            error = "rmse", 
                                            y_init = "LADTree", 
                                            max_depth = 1, niter = 1000,
                                            max_depth_init_set = 1:5,
                                            min_leaf_size_init_set = c(10,20,30),
                                            control = Boost.control(
                                              make_prediction =  TRUE,
                                              cal_imp = TRUE))
```
The parameters of the selected `LADTree` are returned in the 
`params` entry of the returned object:
```{r selected value}
 print(model_RRBoost_cv_LADTree$params)
```
The predictive performance of each of the fits on the 
test set is stored in the `value` entry of the returned objects. 
This is the test error (using the criterion specified with the `error`
argument of the `Boost` call) evaluated 
at the early stopping time, which is determined using the
validation set to avoid overfitting (see paper for more details).  
In this example the returned `value` 
is the test `rmse` evaluated at the early stopping time.  The user
can evaluate more than one prediction performance by passing a
vector of strings to the argument `error`
(e.g. `Boost(..., error=c('rmse', 'rrmse'), ...)`). In that case
the returned `value` will be a 
vector corresponding to those prediction error measures. 

The best prediction performance on the test set was obtained
by selecting a deeper initial `LADTree`:
```{r evaluation}
print(c(median = model_RRBoost_median$value, 
        LADTree = model_RRBoost_LADTree$value,
        cv_LADTree = model_RRBoost_cv_LADTree$value))
```
The variable selection scores are returned in the 
`var_importance` entry. 
```{r variable importance}
 print(cbind(median = model_RRBoost_median$var_importance,
         LADTree = model_RRBoost_LADTree$var_importance,
         cv_LADTree = model_RRBoost_cv_LADTree$var_importance))
```
We note that for the top 2 explanatory variables are
consistently found to be `frequency` and `thickness`.

In the package, we also provide a way that separates training, 
making predictions, and calculating variable importance. It allows 
the flexiblity to make predictions on multiple test sets and 
calculate variable importance when needed. Note that to allow this 
separation, `save_tree = TRUE` is required in `control`. This saves 
the trainer weak learners, which are needed for calculating prediction 
and variable importance, as a model component. 
```{r separate, results='hide', cache=TRUE}
model = Boost(x_train = xtrain, y_train = ytrain, 
              x_val = xval, y_val = yval, 
              # x_test = xtest, y_test = ytest,
              type = "RRBoost", error = "rmse", 
              y_init = "LADTree", max_depth = 1, niter = 1000, 
              control = Boost.control(max_depth_init = 2, 
                                      min_leaf_size_init = 20, 
                                      save_tree = TRUE, 
                                      make_prediction =  FALSE, #TRUE, 
                                      cal_imp = FALSE))

prediction <- cal_predict(model, x_test = xtest, y_test = ytest)
var_importance <-  cal_imp_func(model, x_val = xval, y_val= yval)
```
Sanity check: the results are the same
```{r end}
print(all.equal(prediction$value, model_RRBoost_LADTree$value))
print(all.equal(var_importance, model_RRBoost_LADTree$var_importance))
```
Finally, we compare the performance of 
`RRBoost` with that of the other boosting algorithms implemented
in the package, namely: `L2Boost`, `LADBoost`, `MBoost`, and `Robloss`. 
```{r compare, results="hide", cache=TRUE}
model_L2Boost = Boost(x_train = xtrain, y_train = ytrain, 
                              x_val = xval, y_val = yval, 
                              x_test = xtest, y_test = ytest, 
                              type = "L2Boost", error = "rmse", 
                              y_init = "median", max_depth = 1, 
                              niter = 1000, 
                              control = Boost.control(make_prediction =  TRUE,
                                                      cal_imp = TRUE))
model_LADBoost = Boost(x_train = xtrain, y_train = ytrain, 
                              x_val = xval, y_val = yval, 
                              x_test = xtest, y_test = ytest, 
                              type = "LADBoost", error = "rmse", 
                              y_init = "median", max_depth = 1, 
                              niter = 1000, 
                              control = Boost.control(make_prediction =  TRUE,
                                                      cal_imp = TRUE))
model_MBoost = Boost(x_train = xtrain, y_train = ytrain, 
                              x_val = xval, y_val = yval, 
                              x_test = xtest, y_test = ytest, 
                              type = "MBoost", error = "rmse", 
                              y_init = "median", max_depth = 1, 
                              niter = 1000, 
                              control = Boost.control(make_prediction =  TRUE,
                                                      cal_imp = TRUE))
model_Robloss = Boost(x_train = xtrain, y_train = ytrain, 
                              x_val = xval, y_val = yval, 
                              x_test = xtest, y_test = ytest, 
                              type = "Robloss", error = "rmse", 
                              y_init = "median", max_depth = 1, 
                              niter = 1000, 
                              control = Boost.control(make_prediction =  TRUE,
                                                      cal_imp = TRUE))
```
Note that `RRBoost` yields the best predictions
on the test set: 
```{r compare_val}
print(c( RRBoost = model_RRBoost_cv_LADTree$value,  
         L2Boost = model_L2Boost$value, 
         LADTree = model_LADBoost$value,
         MBoost = model_MBoost$value, 
         Robloss = model_Robloss$value))
```



